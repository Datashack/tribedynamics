{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Vocabularies From Emebddings and Subset Bilingual Vocabularies\n",
    "\n",
    "#### Use this code to extract from source and target vocabularies n words that appear in both \n",
    "#### Note: Skip this part if you data in the folder \"embedding/alignemnt/toy_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read source vocabulary\n",
    "source_vocabulary_embeddings = KeyedVectors.load_word2vec_format(PATH_EMBEDDING_SOURCE)\n",
    "# read target vocabulary\n",
    "target_vocabulary_embeddings = KeyedVectors.load_word2vec_format(PATH_EMBEDDING_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tokens from source and target vocabularies (i.e. words in the vocabulary) \n",
    "words_src = []\n",
    "for word_src in source_vocabulary_embeddings.vocab:\n",
    "    words_src.append(word_src)\n",
    "\n",
    "words_trg = []\n",
    "for word_trg in target_vocabulary_embeddings.vocab:\n",
    "    words_trg.append(word_trg)\n",
    "\n",
    "# Summary Source\n",
    "print(\"Number of Tokens in source: {}\".format(len(words_src)))\n",
    "print(\"Dimension of a word vector source embeddings: {}\".format(len(source_vocabulary_embeddings[words[0]])))\n",
    "\n",
    "# Summary Target\n",
    "print(\"Number of Tokens in target: {}\".format(len(words_it)))\n",
    "print(\"Dimension of a word vector target embeddings: {}\".format(len(target_vocabulary_embeddings[words[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with words index\n",
    "# Source\n",
    "word_to_index_en = {}\n",
    "i=0\n",
    "for word, _ in source_vocabulary_embeddings.vocab.items():\n",
    "    word_to_index_en[word] = i\n",
    "    i=i+1\n",
    "print(\"This is the index of the word you are looking int the source: {}\".format(word_to_index_eng[WORD_TO_CHECK_SOURCE]))\n",
    "\n",
    "# Target\n",
    "word_to_index_it = {}\n",
    "i=0\n",
    "for word, _ in target_vocabulary_embeddings.vocab.items():\n",
    "    word_to_index_it[word] = i\n",
    "    i=i+1\n",
    "print(\"This is the index of the word you are looking int the target: {}\".format(word_to_index_eng[WORD_TO_CHECK_TARGET]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_data(source_vocabulary, target_vocabulary, n_words):\n",
    "    \"\"\"Function to extract bilingual vocabulary from source and target vocabularies. \n",
    "    It returns two dictionaries (i.e. source and target languages) with same keys.\"\"\"\n",
    "\n",
    "    #create a dictionary of overlapping words\n",
    "    source_words = set(source_vocabulary.index2word)\n",
    "    target_words = set(target_vocabulary.index2word)\n",
    "    overlap = list(source_words & target_words)\n",
    "    bilingual_vocabulary = [(entry, entry) for entry in overlap]\n",
    "    \n",
    "    #select a random number of words from the bilingual dictionary\n",
    "    pair_words = random.sample(bilingual_vocabulary, n_words)\n",
    "    toy_words = [tup[0] for tup in pair_words]\n",
    "    \n",
    "    #store vector for each word in two dictionaries (source and target)\n",
    "    source_dictionary = dict()\n",
    "    target_dictionary = dict()\n",
    "    for word in toy_words:\n",
    "        source_dictionary[word] = source_vocabulary.get_vector(str(word))\n",
    "        target_dictionary[word] = target_vocabulary.get_vector(str(word))\n",
    "    \n",
    "    return source_dictionary, target_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate toy data eng_pt\n",
    "source_dictionary, target_dictionary = toy_data(source_vocabulary_embeddings, target_vocabulary_embeddings, 20000)\n",
    "# check if lenght among source and target matches\n",
    "print(\"The two subsets have equal number of entries: {}\".format(len(source_dictionary.keys()) == len(target_dictionary.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data in the folder \"embedding/alignemnt/toy_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two dictionaries have same number of words: True\n"
     ]
    }
   ],
   "source": [
    "# Read from file\n",
    "dictionary_eng_it = np.load('toy_data/toy_data_eng-it_20000.npy').item()\n",
    "dictionary_it_eng = np.load('toy_data/toy_data_it-eng_20000.npy').item()\n",
    "#check if the words are the same in the two vocabulary\n",
    "print(\"The two dictionaries have same number of words: {}\".format(dictionary_eng_it.keys() == dictionary_it_eng.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_toy_data_train_test(source_dictionary, target_dictionary, ratio_train_test):\n",
    "    \"\"\"Function to split source and target dictionaries in train and test. It returns train and test matrices for both source and target languages.\"\"\"\n",
    "    #prepare the matrix\n",
    "    source_matrix = list(source_dictionary.values())\n",
    "    target_matrix = list(target_dictionary.values())\n",
    "    \n",
    "    #select split ratio and random indices\n",
    "    indices = np.random.permutation(len(source_matrix))\n",
    "    ratio_train_test = ratio_train_test\n",
    "    split_range = int(len(source_matrix) * ratio_train_test)\n",
    "    training_idx, test_idx = indices[:split_range], indices[split_range :]\n",
    "\n",
    "    #select by indices train and test for source and target dictionaries\n",
    "    source_train = np.array([source_matrix [i] for i in training_idx])\n",
    "    source_test = np.array([source_matrix[i] for i in test_idx])\n",
    "    target_train = np.array([target_matrix[i] for i in training_idx])\n",
    "    target_test = np.array([target_matrix[i] for i in test_idx])\n",
    "    \n",
    "    return source_train, source_test, target_train, target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train, source_test, target_train, target_test = split_toy_data_train_test(dictionary_eng_it, dictionary_it_eng, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_train.shape, source_test.shape, target_train.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn Transformation to Align Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(transformation, source_test):\n",
    "    \"\"\" Apply the given transformation to the vector space. \n",
    "    It returns predictions given transformations with embeddings E: E = E * transform \"\"\"\n",
    "    return np.matmul(source_test, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn the transformation\n",
    "transformation = learn_transformation(source_train, target_train, normalize_vectors = True)\n",
    "# apply transformation on the test of the source language\n",
    "source_transformed = apply_transform(transformation, source_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_transformed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "    return np.dot(vec_a, vec_b) / \\\n",
    "        (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_proximity(source_transformed, target_test, close_neighbors, metric):\n",
    "    \"\"\"Compute accuracy, cosine similarity and euclidian distance between the closest transformed vectors.\n",
    "    INPUT:\n",
    "    close_neighbors: choose number of neighbors\n",
    "    metric: 'euclidian' or 'cosine' to compute kNN neighbors\n",
    "    source_tranformed: test set of the source language after transformation\n",
    "    target_test: test set of the target language\n",
    "    \n",
    "    OUTPUT:\n",
    "    accuracy: percentage of matching words, within k neighbors, between source and target language after the alignment\n",
    "    cosine_similarity: angolar distance among matching words\n",
    "    l2_norm_distance: euclidian distance among matching words\"\"\"\n",
    "\n",
    "    distance_embeddings = []\n",
    "    accuracy = []\n",
    "    similarity = []\n",
    "    distance = []\n",
    "\n",
    "    #fit model to find the closest vectors to tranformed one\n",
    "    neighbors = NearestNeighbors(n_neighbors = close_neighbors, metric = metric)\n",
    "    neighbors.fit(target_test)\n",
    "\n",
    "    for i in range(0,source_transformed.shape[0]):\n",
    "        index_transformed = i\n",
    "        #distance between two vocabularies\n",
    "        distance_embeddings.append(np.linalg.norm(source_transformed[i] - target_test[i]))\n",
    "        #find vectors in the target vocabulary\n",
    "        idx_neighbors_target = ((neighbors.kneighbors(source_transformed[index_transformed,:].reshape(1, -1)))[1][0]).tolist()\n",
    "        #evaluate proximity among target and transformed vectors\n",
    "        if index_transformed in idx_neighbors_target:\n",
    "            accuracy.append('True')\n",
    "            similarity.append(cosine_similarity(source_transformed[index_transformed], target_test[index_transformed]))\n",
    "            distance.append(np.linalg.norm(source_transformed[index_transformed] - target_test[index_transformed]))\n",
    "        else:\n",
    "            accuracy.append('False')\n",
    "\n",
    "    return [np.mean(distance_embeddings), round(accuracy.count('True')/len(accuracy)*100,2), np.mean(similarity), np.mean(distance)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(source_test, source_transformed, target_test):\n",
    "    \"\"\"Evalaute alignment comparing the source vocabulary and its transformed version with the target vocabulary.\n",
    "    INPUT:\n",
    "    source_test: vocabulary of the source language\n",
    "    source_transformed: test set of the source language after transformation\n",
    "    target_test: test set of the target language\n",
    "    \n",
    "    OUTPUT:\n",
    "    dictionaries evaluating the alignment with different metrics\"\"\"\n",
    "    \n",
    "    sources = [source_test, source_transformed]\n",
    "    metrics = ['cosine', 'euclidean']\n",
    "    neighbors = [1,5,10]\n",
    "    evaluation_before_cosine = dict()\n",
    "    evaluation_before_euclidean = dict()\n",
    "    evaluation_after_cosine = dict()\n",
    "    evaluation_after_euclidean = dict()\n",
    "\n",
    "    for source in sources:\n",
    "        if source[0][0] == source_test[0][0]:\n",
    "            for metric in metrics:\n",
    "                if metric == 'cosine':\n",
    "                    for neighbor in neighbors:\n",
    "                        evaluation_before_cosine[neighbor] = evaluate_proximity(source_test, target_test, neighbor, 'cosine')\n",
    "\n",
    "                else:\n",
    "                    for neighbor in neighbors:\n",
    "                        evaluation_before_euclidean[neighbor] = evaluate_proximity(source_test, target_test, neighbor, 'euclidean')\n",
    "\n",
    "        else:\n",
    "            for metric in metrics:\n",
    "                if metric == 'cosine':\n",
    "                    for neighbor in neighbors:\n",
    "                        evaluation_after_cosine[neighbor] = evaluate_proximity(source_transformed, target_test, neighbor, 'cosine')\n",
    "\n",
    "                else:\n",
    "                    for neighbor in neighbors:\n",
    "                        evaluation_after_euclidean[neighbor] = evaluate_proximity(source_transformed, target_test, neighbor, 'euclidean') \n",
    "\n",
    "    return evaluation_before_cosine, evaluation_before_euclidean, evaluation_after_cosine, evaluation_after_euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate alignment source target\n",
    "evaluation_before_cosine, evaluation_before_euclidean, evaluation_after_cosine, evaluation_after_euclidean = evaluate(source_test, source_transformed, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set alignment evaluation table\n",
    "index = ['distance_embeddings', 'accuracy', 'cosine_similarity', 'euclidean_distance']\n",
    "\n",
    "#dump evaluation within dataframe\n",
    "df_eng_it = pd.DataFrame({'a': list(evaluation_before_cosine[1]),\n",
    "                          'b': list(evaluation_after_cosine[1]),\n",
    "                          'c': list(evaluation_before_euclidean[1]),\n",
    "                          'd': list(evaluation_after_euclidean[1]),\n",
    "                          \n",
    "                          'e': list(evaluation_before_cosine[5]),\n",
    "                          'f': list(evaluation_after_cosine[5]),\n",
    "                          'g': list(evaluation_before_euclidean[5]),\n",
    "                          'h': list(evaluation_after_euclidean[5]),\n",
    "          \n",
    "                          'i': list(evaluation_before_cosine[10]),\n",
    "                          'l': list(evaluation_after_cosine[10]),\n",
    "                          'm': list(evaluation_before_euclidean[10]),\n",
    "                          'n': list(evaluation_after_euclidean[10])}, index=index)\n",
    "\n",
    "#assign multilevel temporary columns name\n",
    "columns=[('K@1','COSINE', 'a'),  ('K@1','COSINE', 'b'), ('K@1','EUCLIDEAN', 'c'),  ('K@1','EUCLIDEAN', 'd'),\n",
    "   ('K@5','COSINE', 'e'),  ('K@5','COSINE', 'f'), ('K@5','EUCLIDEAN', 'g'),  ('K@5','EUCLIDEAN', 'h'),\n",
    "    ('K@10','COSINE', 'i'),  ('K@10','COSINE', 'l'), ('K@10','EUCLIDEAN', 'm'),  ('K@10','EUCLIDEAN', 'n')]\n",
    "\n",
    "#make multindex\n",
    "df_eng_it.columns= pd.MultiIndex.from_tuples(columns)\n",
    "\n",
    "#change columns names\n",
    "change_columns_name = ['non_tranformed','tranformed','non_tranformed','tranformed', 'non_tranformed','tranformed','non_tranformed','tranformed', 'non_tranformed','tranformed','non_tranformed','tranformed']\n",
    "df_eng_it.columns.set_levels(change_columns_name,level=2,inplace=True)\n",
    "df_eng_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not run if not needed\n",
    "#save evaluation table to csv\n",
    "df_eng_it.to_csv('evaluation_embedding_it_eng_30k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read evaluation\n",
    "df = pd.read_csv('evaluation_eng_pt_20k.csv', header=[0,1,2], tupleize_cols=True)\n",
    "df.columns = pd.MultiIndex.from_tuples(df.columns)\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rotation(source_test, source_transformed, target_test):\n",
    "    \"\"\"Evalaute rotation comparing: 1) the source vocabulary and its transformed version;\n",
    "                                    2) source vocabulary transformed and the target vocabulary;\n",
    "                                    3) source and target vocabularies;\n",
    "    INPUT:\n",
    "    source_test: vocabulary of the source language\n",
    "    source_transformed: test set of the source language after transformation\n",
    "    target_test: test set of the target language\n",
    "    \n",
    "    OUTPUT:\n",
    "    euclidian distance and cosine similarity for each rotation comparison\"\"\"\n",
    "\n",
    "    distance_source_target = []\n",
    "    distance_source_transformed = []\n",
    "    distance_transformed_target = []\n",
    "    similarity_source_target = []\n",
    "    similarity_source_transformed = []\n",
    "    similarity_transformed_target = []\n",
    "\n",
    "    for i in range(0,source_test.shape[0]):\n",
    "        distance_source_target.append(np.linalg.norm(source_test[i] - target_test[i]))\n",
    "        distance_source_transformed.append(np.linalg.norm(source_test[i] - source_transformed[i]))\n",
    "        distance_transformed_target.append(np.linalg.norm(source_transformed[i] - target_test[i]))\n",
    "\n",
    "        similarity_source_target.append(cosine_similarity(target_test[i],source_test[i]))\n",
    "        similarity_source_transformed.append(cosine_similarity(source_test[i], source_transformed[i]))\n",
    "        similarity_transformed_target.append(cosine_similarity(source_transformed[i], target_test[i]))\n",
    "\n",
    "    return distance_source_target, distance_source_transformed, distance_transformed_target, similarity_source_target, similarity_source_transformed, similarity_transformed_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalutate rotation\n",
    "distance_source_target, distance_source_transformed, distance_transformed_target, similarity_source_target, similarity_source_transformed, similarity_transformed_target = evaluate_rotation(source_test, source_transformed, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store output evaluation rotation in a dictionary\n",
    "output_evaluation_rotation = {'distance_source_target':distance_source_target, \n",
    "                              'distance_source_transformed': distance_source_transformed, \n",
    "                              'distance_transformed_target': distance_transformed_target, \n",
    "                              'similarity_source_target': similarity_source_target, \n",
    "                              'similarity_source_transformed': similarity_source_transformed, \n",
    "                              'similarity_transformed_target': similarity_transformed_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rememeber first language is the target, second the source\n",
    "# save evaluation results \n",
    "import pickle\n",
    "with open('output_evaluation_rotation_pt_eng', 'wb') as f:\n",
    "    pickle.dump(output_evaluation_rotation, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load evaluation results\n",
    "with open('output_evaluation_rotation_eng_pt', 'rb') as f:\n",
    "     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "main_ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "main_ax.hist(distance_source_target, \n",
    "             color = \"lightcoral\", bins=40, alpha=.5, \n",
    "             edgecolor='black', linewidth=1, label='Source to Target')\n",
    "main_ax.hist(distance_transformed_target, \n",
    "             color = \"lightskyblue\", bins=40, alpha=0.6, \n",
    "             edgecolor='black', linewidth=1, label='Transformed to Target')\n",
    "main_ax.axvline(np.mean(distance_source_target)-.05, color='black',alpha=0.7, linestyle='dashed', linewidth=1.5)\n",
    "main_ax.axvline(np.mean(distance_transformed_target), color='black',alpha=0.7, linestyle='dashed', linewidth=1.5)\n",
    "\n",
    "#main_ax.set_xticklabels(fontsize=15)\n",
    "#main_ax.set_yticklabels(fontsize=15)\n",
    "fs = 18\n",
    "\n",
    "plt.xlabel('Euclidean Distance', fontsize=fs)\n",
    "plt.ylabel('Frequency', fontsize=fs)\n",
    "\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "\n",
    "\n",
    "plt.legend(loc='upper right',fontsize=fs)\n",
    "#plt.figlegend(loc = 'upper right', ncol=3, labelspacing=0.5) #bbox_to_anchor=(1.1, 1.05)\n",
    "#plt.suptitle('Euclidean Distance:\\n before and after alignment comparisons\\nEnglish-Italian', fontsize=15)\n",
    "plt.savefig('Euclidean_Distance_word2vec_ENG_IT_Poster.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot euclidian distances comparison pt-eng\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "grid = plt.GridSpec(24, 4, hspace=0.5, wspace=0.5)\n",
    "main_ax = fig.add_subplot(grid[:16, 0:])\n",
    "x1_hist = fig.add_subplot(grid[-7:, :2], sharex=main_ax)\n",
    "x2_hist = fig.add_subplot(grid[-7:, 2:], sharex=main_ax)\n",
    "\n",
    "main_ax.hist(distance_source_target, \n",
    "             color = \"lightcoral\", bins=40, alpha=.5, \n",
    "             edgecolor='black', linewidth=1, label='distance_source_target')\n",
    "main_ax.hist(distance_transformed_target, \n",
    "             color = \"lightskyblue\", bins=40, alpha=0.6, \n",
    "             edgecolor='black', linewidth=1, label='distance_transformed_target')\n",
    "main_ax.axvline(np.mean(distance_source_target)-.05, color='black',alpha=0.7, linestyle='dashed', linewidth=1.5)\n",
    "main_ax.axvline(np.mean(distance_transformed_target), color='black',alpha=0.7, linestyle='dashed', linewidth=1.5)\n",
    "#main_ax.annotate(str(round(np.mean(distance_transformed_target),2)), xy=(2, 820), xytext=(4.55, 815))\n",
    "\n",
    "#ok\n",
    "x1_hist.hist(distance_source_target, \n",
    "             color = \"lightcoral\", bins=40, alpha=0.4, \n",
    "             edgecolor='black', linewidth=.5) #, label='source_target'\n",
    "x1_hist.hist(distance_source_transformed, \n",
    "             color = \"seagreen\", bins=40, alpha=0.5, \n",
    "             edgecolor='black', linewidth=.5, label='distance_source_transformed')\n",
    "x1_hist.axvline(np.mean(distance_source_target), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "x1_hist.axvline(np.mean(distance_source_transformed), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "\n",
    "x2_hist.hist(distance_source_transformed, \n",
    "             color = \"seagreen\", bins=40, alpha=0.5,\n",
    "             edgecolor='black', linewidth=.5) #, label='source_transformed'\n",
    "x2_hist.hist(distance_transformed_target, \n",
    "             color = \"lightskyblue\", bins=40, alpha=0.6, \n",
    "             edgecolor='black', linewidth=.5) #, label='transformed_target'\n",
    "x2_hist.axvline(np.mean(distance_source_transformed), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "x2_hist.axvline(np.mean(distance_transformed_target), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "\n",
    "plt.figlegend(loc = 'lower center', ncol=3, labelspacing=0.5) #bbox_to_anchor=(1.1, 1.05)\n",
    "plt.suptitle('Euclidean Distance:\\n before and after alignment comparisons\\nEnglish-Italian', fontsize=15)\n",
    "\n",
    "plt.savefig('Euclidean_Distance_Word_Embeddings_word2vec_ENG_IT_30k.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot euclidian distances comparison eng-pt\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "grid = plt.GridSpec(24, 4, hspace=0.5, wspace=0.5)\n",
    "main_ax = fig.add_subplot(grid[:16, 0:])\n",
    "x1_hist = fig.add_subplot(grid[-7:, :2], sharex=main_ax)\n",
    "x2_hist = fig.add_subplot(grid[-7:, 2:], sharex=main_ax)\n",
    "\n",
    "main_ax.hist(distance_source_target, \n",
    "             color = \"lightcoral\", bins=40, alpha=.5, \n",
    "             edgecolor='black', linewidth=1, label='distance_source_target')\n",
    "main_ax.hist(distance_transformed_target, \n",
    "             color = \"lightskyblue\", bins=40, alpha=0.6, \n",
    "             edgecolor='black', linewidth=1, label='distance_transformed_target')\n",
    "main_ax.axvline(np.mean(distance_source_target)-.05, color='black',alpha=0.7, linestyle='dashed', linewidth=1.5)\n",
    "main_ax.axvline(np.mean(distance_transformed_target), color='black',alpha=0.7, linestyle='dashed', linewidth=1.5)\n",
    "#main_ax.annotate(str(round(np.mean(distance_transformed_target),2)), xy=(2, 820), xytext=(4.55, 815))\n",
    "\n",
    "#ok\n",
    "x1_hist.hist(distance_source_target, \n",
    "             color = \"lightcoral\", bins=40, alpha=0.4, \n",
    "             edgecolor='black', linewidth=.5) #, label='source_target'\n",
    "x1_hist.hist(distance_source_transformed, \n",
    "             color = \"seagreen\", bins=40, alpha=0.5, \n",
    "             edgecolor='black', linewidth=.5, label='distance_source_transformed')\n",
    "x1_hist.axvline(np.mean(distance_source_target), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "x1_hist.axvline(np.mean(distance_source_transformed), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "\n",
    "x2_hist.hist(distance_source_transformed, \n",
    "             color = \"seagreen\", bins=40, alpha=0.5,\n",
    "             edgecolor='black', linewidth=.5) #, label='source_transformed'\n",
    "x2_hist.hist(distance_transformed_target, \n",
    "             color = \"lightskyblue\", bins=40, alpha=0.6, \n",
    "             edgecolor='black', linewidth=.5) #, label='transformed_target'\n",
    "x2_hist.axvline(np.mean(distance_source_transformed), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "x2_hist.axvline(np.mean(distance_transformed_target), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "\n",
    "plt.figlegend(loc = 'lower center', ncol=3, labelspacing=0.5) #bbox_to_anchor=(1.1, 1.05)\n",
    "#plt.suptitle('Euclidean Distance:\\n before and after alignment comparisons\\nEnglish-Portuguese', fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cosine similarities distances\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "grid = plt.GridSpec(24, 4, hspace=0.5, wspace=0.5)\n",
    "main_ax = fig.add_subplot(grid[:16, 0:])\n",
    "x1_hist = fig.add_subplot(grid[-7:, :2], sharex=main_ax)\n",
    "x2_hist = fig.add_subplot(grid[-7:, 2:], sharex=main_ax)\n",
    "\n",
    "main_ax.hist(similarity_source_target, \n",
    "             color = \"lightcoral\", bins=40, alpha=.5, \n",
    "             edgecolor='black', linewidth=1, label='similarity_source_target')\n",
    "main_ax.hist(similarity_transformed_target, \n",
    "             color = \"lightskyblue\", bins=40, alpha=0.6, \n",
    "             edgecolor='black', linewidth=1, label='similarity_transformed_target')\n",
    "main_ax.axvline(np.mean(similarity_source_target)-.05, color='black',alpha=0.7, linestyle='dashed', linewidth=1.5)\n",
    "main_ax.axvline(np.mean(similarity_transformed_target), color='black',alpha=0.7, linestyle='dashed', linewidth=1.5)\n",
    "#main_ax.annotate(str(round(np.mean(distance_transformed_target),2)), xy=(2, 820), xytext=(4.55, 815))\n",
    "\n",
    "x1_hist.hist(similarity_source_target, \n",
    "             color = \"lightcoral\", bins=20, alpha=0.4, \n",
    "             edgecolor='black', linewidth=.5) #, label='source_target'\n",
    "x1_hist.hist(similarity_source_transformed, \n",
    "             color = \"seagreen\", bins=20, alpha=0.5, \n",
    "             edgecolor='black', linewidth=.5, label='similarity_source_transformed')\n",
    "x1_hist.axvline(np.mean(similarity_source_target), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "x1_hist.axvline(np.mean(similarity_source_transformed), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "\n",
    "x2_hist.hist(similarity_source_transformed, \n",
    "             color = \"seagreen\", bins=20, alpha=0.5,\n",
    "             edgecolor='black', linewidth=.5) #, label='source_transformed'\n",
    "x2_hist.hist(similarity_transformed_target, \n",
    "             color = \"lightskyblue\", bins=20, alpha=0.6, \n",
    "             edgecolor='black', linewidth=.5) #, label='transformed_target'\n",
    "x2_hist.axvline(np.mean(similarity_source_transformed), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "x2_hist.axvline(np.mean(similarity_transformed_target), color='black',alpha=0.5, linestyle='dashed', linewidth=1.3)\n",
    "\n",
    "plt.figlegend(loc = 'lower center', ncol=3, labelspacing=0.5) #bbox_to_anchor=(1.1, 1.05)\n",
    "plt.suptitle('Cosine Similarity:\\n before and after alignment comparisons\\nItalian-English', fontsize=15)\n",
    "#plt.savefig('Cosine_Similarity_Word_Embeddings_word2vec_ENG_IT_30k.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "main_ax = fig.add_subplot(111)\n",
    "\n",
    "main_ax.hist(similarity_source_target, \n",
    "             color = \"lightcoral\", bins=40, alpha=.5, \n",
    "             edgecolor='black', linewidth=1, label='Source to Targe')\n",
    "main_ax.hist(similarity_transformed_target, \n",
    "             color = \"lightskyblue\", bins=40, alpha=0.6, \n",
    "             edgecolor='black', linewidth=1, label='Transformed to Target')\n",
    "main_ax.axvline(np.mean(similarity_source_target)-.05, color='black',alpha=0.7, linestyle='dashed', linewidth=1.5)\n",
    "main_ax.axvline(np.mean(similarity_transformed_target), color='black',alpha=0.7, linestyle='dashed', linewidth=1.5)\n",
    "\n",
    "#main_ax.set_xticklabels(fontsize=15)\n",
    "#main_ax.set_yticklabels(fontsize=15)\n",
    "fs = 18\n",
    "\n",
    "plt.xlabel('Cosine Similarity', fontsize=fs)\n",
    "plt.ylabel('Frequency', fontsize=fs)\n",
    "\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "\n",
    "plt.legend(loc='upper right',fontsize=fs)\n",
    "#plt.figlegend(loc = 'best', ncol=1, labelspacing=0.5,fontsize=fs) #bbox_to_anchor=(1.1, 1.05)\n",
    "#plt.suptitle('Euclidean Distance:\\n before and after alignment comparisons\\nEnglish-Italian', fontsize=15)\n",
    "plt.savefig('Cosine_Similarity_word2vec_ENG_IT_30k_Poster.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_eng = apply_transform(transformation, source_vocabulary_embeddings.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transformed_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_en = []\n",
    "for word_en in source_vocabulary_embeddings.vocab:\n",
    "    words_en.append(words_en)\n",
    "    \n",
    "vectors_en = []\n",
    "for vector_en in transformed_eng:\n",
    "    vectors_en.append(vector_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_en) == len(vectors_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_vocabulary_embeddings.wv.vectors[0][1])\n",
    "print(vectors_en[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectors_en) == len(words_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rememeber first language is the target, second the source\n",
    "# save evaluation results \n",
    "import pickle\n",
    "with open('en_embeddings_transformed_vectors_30k', 'wb') as f:\n",
    "    pickle.dump(vectors_en, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rememeber first language is the target, second the source\n",
    "# save evaluation results \n",
    "import pickle\n",
    "with open('en_embeddings_transformed_words_30k', 'wb') as f:\n",
    "    pickle.dump(words_en, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_transformed_1 = dict(zip(words_en, vectors_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    list_data_items.append(dict(zip(column_names, item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_transformed['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_it = apply_transform(transformation, target_vocabulary_embeddings.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_it = []\n",
    "for word_it in target_vocabulary_embeddings.vocab:\n",
    "    words_it.append(word_it)\n",
    "    \n",
    "vectors_it = []\n",
    "for vector_it in transformed_it:\n",
    "    vectors_it.append(vector_it)\n",
    "    \n",
    "it_embeddings_transformed = dict(zip(words_it, vectors_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_vocabulary_embeddings.wv.vectors[0][0])\n",
    "print(vectors_it[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rememeber first language is the target, second the source\n",
    "# save evaluation results \n",
    "import pickle\n",
    "with open('it_embeddings_transformed_30k', 'wb') as f:\n",
    "    pickle.dump(it_embeddings_transformed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load evaluation results\n",
    "import pickle\n",
    "with open('it_embeddings_transformed_30k', 'rb') as f:\n",
    "     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['finestra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Word2Vec()\n",
    "model.build_vocab_from_freq(it_embeddings_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_word2vec_format(fname, prefix='*dt_', fvocab=None, total_vec=None, binary=False, write_first_line=True)¶\n",
    "Store the input-hidden weight matrix in the same format used by the original C word2vec-tool, for compatibility.\n",
    "\n",
    "Parameters:\t\n",
    "fname (str) – The file path used to save the vectors in.\n",
    "prefix (str) – Uniquely identifies doctags from word vocab, and avoids collision in case of repeated string in doctag and word vocab.\n",
    "fvocab (str) – Optional file path used to save the vocabulary\n",
    "binary (bool) – If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
    "total_vec (int) – Optional parameter to explicitly specify total no. of vectors (in case word vectors are appended with document vectors afterwards)\n",
    "write_first_line (bool) – Whether to print the first line in the file. Useful when saving doc-vectors after word-vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_word2vec_format(fname, fvocab=None, binary=False, total_vec=None)\n",
    "Store the input-hidden weight matrix in the same format used by the original C word2vec-tool, for compatibility.\n",
    "\n",
    "Parameters:\t\n",
    "fname (str) – The file path used to save the vectors in.\n",
    "fvocab (str) – Optional file path used to save the vocabulary.\n",
    "binary (bool) – If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
    "total_vec (int) – Optional parameter to explicitly specify total no. of vectors (in case word vectors are appended with document vectors afterwards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot t-sne (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit number of tokens to be visualized\n",
    "limit = 150\n",
    "vector_dim = 300\n",
    "\n",
    "# Getting tokens and vectors\n",
    "words = []\n",
    "embedding = np.array([])\n",
    "i = 0\n",
    "for word in it_dictionary.vocab:\n",
    "    # Break the loop if limit exceeds \n",
    "    if i == limit: break\n",
    "\n",
    "    # Getting token \n",
    "    words.append(word)\n",
    "\n",
    "    # Appending the vectors \n",
    "    embedding = np.append(embedding, it_dictionary[word])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# Reshaping the embedding vector \n",
    "embedding = embedding.reshape(limit, vector_dim)\n",
    "\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='it_dictionary_tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "# Creating the tsne plot\n",
    "tsne = TSNE(perplexity=30.0, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "low_dim_embedding = tsne.fit_transform(embedding)\n",
    "\n",
    "# Finally plotting and saving the fig \n",
    "plot_with_labels(low_dim_embedding, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit number of tokens to be visualized\n",
    "limit = 500\n",
    "vector_dim = 300\n",
    "\n",
    "# Getting tokens and vectors\n",
    "words = []\n",
    "embedding = np.array([])\n",
    "i = 0\n",
    "for word in eng_dictionary.vocab:\n",
    "    # Break the loop if limit exceeds \n",
    "    if i == limit: break\n",
    "\n",
    "    # Getting token \n",
    "    words.append(word)\n",
    "\n",
    "    # Appending the vectors \n",
    "    embedding = np.append(embedding, eng_dictionary[word])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# Reshaping the embedding vector \n",
    "embedding = embedding.reshape(limit, vector_dim)\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='eng_dictionary_tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "# Creating the tsne plot [Warning: will take time]\n",
    "tsne = TSNE(perplexity=30.0, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "low_dim_embedding = tsne.fit_transform(embedding)\n",
    "\n",
    "# Finally plotting and saving the fig \n",
    "plot_with_labels(low_dim_embedding, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename='it_dictionary_tsne_3d.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y, z = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "# Creating the tsne plot [Warning: will take time]\n",
    "tsne = TSNE(perplexity=30.0, n_components=3, init='pca', n_iter=5000)\n",
    "\n",
    "low_dim_embedding = tsne.fit_transform(embedding)\n",
    "\n",
    "# Finally plotting and saving the fig \n",
    "plot_with_labels(low_dim_embedding, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
