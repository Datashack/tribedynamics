{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# to transform words in vectors and to build and organize models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# to process words\n",
    "# to download nltk packages de-comment next line\n",
    "# nltk.download() \n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.snowball import ItalianStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'brand_id', 'worker_id', 'mturker',\n",
       "       'post_hash', 'answer', 'date', 'duration_seconds', 'text',\n",
       "       'model_decision', 'timestamped_model', 'lang'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "src = pd.read_csv('../../Data/tribe_dynamics_data.csv')\n",
    "src.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert False to 0s and True to 1s\n",
    "src['answer'] = (src['answer'] == True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9895, 3) (469, 3) (9426, 3)\n"
     ]
    }
   ],
   "source": [
    "# select posts language (i.e. english and italian)\n",
    "src=src[['answer','text','lang']]\n",
    "df_all = src[(src.lang == 'en') | (src.lang == 'it')]\n",
    "df_ita = src[src['lang'] == 'it']\n",
    "df_eng = src[src['lang'] == 'en']\n",
    "print(df_all.shape, df_ita.shape, df_eng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomely extract a data sample of same name observation as other dataframe\n",
    "df_eng_reduced = df_eng.sample(n=df_ita.shape[0])\n",
    "df_eng_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469, 3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#divide df in english among true and false\n",
    "df_eng_true = df_eng[df_eng['answer'] == True]\n",
    "df_eng_false = df_eng[df_eng['answer'] == False]\n",
    "#randomely select true and false as in the dataframe of posts in italian\n",
    "df_eng_reduced_true = df_eng_true.sample(n=df_ita[df_ita['answer'] == True].shape[0])\n",
    "df_eng_reduced_false = df_eng_false.sample(n=df_ita[df_ita['answer'] == False].shape[0])\n",
    "#combine true and false of the reduced dataframe\n",
    "df_eng_reduced_weighted = pd.concat([df_eng_reduced_true, df_eng_reduced_false])\n",
    "df_eng_reduced_weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert text and classifier in arrays\n",
    "X_all = np.asarray(df_all['text'])\n",
    "Y_all = np.asarray(df_all['answer'], dtype=\"|S6\")\n",
    "X_eng = np.asarray(df_eng['text'])\n",
    "Y_eng = np.asarray(df_eng['answer'], dtype=\"|S6\")\n",
    "X_eng_reduced = np.asarray(df_eng_reduced['text'])\n",
    "Y_eng_reduced = np.asarray(df_eng_reduced['answer'], dtype=\"|S6\")\n",
    "X_eng_reduced_weighted = np.asarray(df_eng_reduced_weighted['text'])\n",
    "Y_eng_reduced_weighted = np.asarray(df_eng_reduced_weighted['answer'], dtype=\"|S6\")\n",
    "X_ita = np.asarray(df_ita['text'])\n",
    "Y_ita = np.asarray(df_ita['answer'], dtype=\"|S6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split in train and test\n",
    "X_train_all, X_test_all, Y_train_all, Y_test_all = train_test_split(X_all, Y_all, test_size=0.4, random_state=42)\n",
    "X_train_eng, X_test_eng, Y_train_eng, Y_test_eng = train_test_split(X_eng, Y_eng, test_size=0.4, random_state=42)\n",
    "X_train_eng_reduced, X_test_eng_reduced, Y_train_eng_reduced, Y_test_eng_reduced = train_test_split(X_eng_reduced, Y_eng_reduced, test_size=0.4, random_state=42)\n",
    "X_train_eng_reduced_weighted, X_test_eng_reduced_weighted, Y_train_eng_reduced_weighted, Y_test_eng_reduced_weighted = train_test_split(X_eng_reduced_weighted, Y_eng_reduced_weighted, test_size=0.4, random_state=42)\n",
    "X_train_ita, X_test_ita, Y_train_ita, Y_test_ita = train_test_split(X_ita, Y_ita, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'0' b'8626']\n",
      " [b'1' b'800']]\n"
     ]
    }
   ],
   "source": [
    "# Posts written in English\n",
    "# number of datapoints labeled False and True\n",
    "unique, counts = np.unique(Y_eng, return_counts=True)\n",
    "print (np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'0' b'5156']\n",
      " [b'1' b'499']]\n"
     ]
    }
   ],
   "source": [
    "# Posts written in English : train set\n",
    "unique, counts = np.unique(Y_train_eng, return_counts=True)\n",
    "print (np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'0' b'428']\n",
      " [b'1' b'41']]\n"
     ]
    }
   ],
   "source": [
    "# Posts written in English from reduced dataframe: train set\n",
    "unique, counts = np.unique(Y_eng_reduced, return_counts=True)\n",
    "print (np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'0' b'276']\n",
      " [b'1' b'5']]\n"
     ]
    }
   ],
   "source": [
    "# Posts written in English from reduced dataframe with same amount True and False as the other dataframe: train set\n",
    "unique, counts = np.unique(Y_train_eng_reduced_weighted, return_counts=True)\n",
    "print (np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'0' b'273']\n",
      " [b'1' b'8']]\n"
     ]
    }
   ],
   "source": [
    "# Posts written in Italian: train set\n",
    "# number of datapoints labeled False and True\n",
    "unique, counts = np.unique(Y_train_ita, return_counts=True)\n",
    "print (np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bag of words for english posts\n",
    "# this class create steems words and ignore stopwords\n",
    "# using CountVectorizer methods each unique word in the dictionary will correspond to a feature\n",
    "stemmer_eng = SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer_eng.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vect_eng = StemmedCountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run ML algorithm\n",
    "# build pipeline for Naive Bayes classifier\n",
    "# control for Term Frequencies (i.e. #count(word) / #Total words)\n",
    "# control for Term Frequency times inverse document frequency (i.e. weightage of more common words like the, is, an etc.) \n",
    "text_stemmed_eng = Pipeline([('vect', stemmed_count_vect_eng),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('mnb', MultinomialNB(fit_prior=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93450013259082476"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance of NB Classifier for posts in english\n",
    "text_stemmed_eng = text_stemmed_eng.fit(X_train_eng, Y_train_eng)\n",
    "predicted_stemmed_eng = text_stemmed_eng.predict(X_test_eng)\n",
    "np.mean(predicted_stemmed_eng == Y_test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91489361702127658"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance of NB Classifier for posts in english dataframe reduced\n",
    "text_stemmed_eng_reduced = text_stemmed_eng.fit(X_train_eng_reduced, Y_train_eng_reduced)\n",
    "predicted_stemmed_eng_reduced = text_stemmed_eng.predict(X_test_eng_reduced)\n",
    "np.mean(predicted_stemmed_eng_reduced == Y_test_eng_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97340425531914898"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance of NB Classifier for posts in english dataframe reduced\n",
    "text_stemmed_eng_reduced_weighted = text_stemmed_eng.fit(X_train_eng_reduced_weighted, Y_train_eng_reduced_weighted)\n",
    "predicted_stemmed_eng_reduced_weighted = text_stemmed_eng.predict(X_test_eng_reduced_weighted)\n",
    "np.mean(predicted_stemmed_eng_reduced_weighted == Y_test_eng_reduced_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bag of words for italian posts\n",
    "stemmer_ita = ItalianStemmer(ignore_stopwords=False)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer_ita.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vect_ita = StemmedCountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build pipeline for Naive Bayes classifier\n",
    "text_stemmed_ita = Pipeline([('vect', stemmed_count_vect_ita),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('mnb', MultinomialNB(fit_prior=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98936170212765961"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance of NB Classifier for posts in italian\n",
    "text_stemmed_ita = text_stemmed_ita.fit(X_train_ita, Y_train_ita)\n",
    "predicted_stemmed_ita = text_stemmed_ita.predict(X_test_ita)\n",
    "np.mean(predicted_stemmed_ita == Y_test_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid Search for english posts\n",
    "# List of parameters for which we would like to do performance tuning. \n",
    "# vect__ngram_range is telling to use unigram and bigrams and choose the optimal\n",
    "parameters_eng = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "# parameters and n_jobs=-1 which tells to use multiple cores from user machine\n",
    "gs_clf_eng = GridSearchCV(text_stemmed_eng, parameters_eng, n_jobs=-1)\n",
    "gs_clf_eng = gs_clf_eng.fit(X_train_eng, Y_train_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measure performance for english posts\n",
    "print(gs_clf_eng.best_score_)\n",
    "print(gs_clf_eng.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid Search for italian posts\n",
    "parameters_ita = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],  #from unigrams to fourgrams\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf_ita = GridSearchCV(text_ita_stemmed, parameters_ita, n_jobs=-1)\n",
    "gs_clf_ita = gs_clf_ita.fit(X_train_ita, Y_train_ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measure performance for italian posts\n",
    "print(gs_clf_ita.best_score_)\n",
    "print(gs_clf_ita.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Future improvements: TODO\n",
    "\n",
    "# implement a more general class StemmedCountVectorizer allowing for multiple languages \n",
    "# try with one bag of words and one classifier for both languages\n",
    "# try with a sample of the same size (50:50), and with different size (80:20)\n",
    "# try Gaussian, and Bernoulli Naive Bayes\n",
    "# Smooth parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
